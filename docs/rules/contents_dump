################
# filename: cheatsheet.md
################
# [Financial Math](./fin_math.md)

* Never weight log returns across assets; Always compute portfolio period return with simple returns (s_rt) then convert to log_rt if needed.
* Never compound across time with products of (1 + s_rt) on long spans; Always sum log_rt and use expm1/exp to get back to simple space.
* Never mix return types implicitly; Always use explicit names: s_rt (simple), log_rt (log).
* Never use np.log(1 + x) or np.exp(x) - 1; Always use np.log1p(x) and np.expm1(x) for stability.
* Never compute returns from unadjusted prices; Always use split/dividend-adjusted prices or add distributions for total return.
* Never forward-fill returns; Always leave NaNs and mask/renormalize weights when data is missing.
* Never assume alignment or local time; Always align indices explicitly and use tz-aware UTC timestamps.
* Never store ledger money as floats; Always use integers in minor units (e.g., cents) or Decimal with fixed quantization (analytics stay float64).
* Never show internal log metrics to users; Always report in simple terms (period s_rt, CAGR) with clear labels.
* Never annualize by arithmetic mean of s_rt × K; Always annualize via mean log_rt then exp(mean_log*K) - 1.
* Never compute volatility with ddof=0; Always use ddof=1 and scale by sqrt(K).
* Never subtract a simple risk-free from log_rt; Always convert RF to per-period log and subtract in log space.
* Never let weights drift without clarity or fail to sum to 1; Always use start-of-period weights, normalize within tolerance, and label series rebalanced vs drifted.
* Never ignore the log1p domain (x > -1); Always guard with epsilon and check finiteness after transforms.
* Never round during analytics; Always round/quantize only at I/O boundaries.
* Never hardcode frequency blindly or mix frequencies; Always carry a frequency tag and use the correct annualization factor (K=252/365/52/12 as policy).
* Never proceed with infs/NaNs silently; Always validate (np.isfinite) and halt or apply an explicit, documented imputation policy.
* Never apply fees/slippage/taxes in log space; Always subtract costs as simple rates at the period, then convert if needed.
* Never rely on implicit index matching for weights/returns; Always assert aligned shapes/indices before arithmetic.
* Never leave compounding semantics ambiguous; Always state the interval convention (left-close/right-open) and what each return maps to (t-1, t]

# [Python](./python.md)

* Never modify `sys.path`; Always place imports at the top, grouped stdlib/third-party/local.
* Never use dynamic, nested, or try/except imports; Always declare explicit dependencies.
* Never use `from __future__ import annotations`; Always use builtin types (e.g., `list[str] | None`).
* Never mix refactors with features; Always separate them into distinct commits.
* Never change behavior without traceability; Always record breaking changes in **CHANGELOG.md › Unreleased** and label the PR.
* Never reference ephemeral or untracked files; Always reference paths present in `git ls-files`.
* Never sprinkle feature flags throughout code; Always read flags once at the edge and inject inward.
* Never organize code by convenience-only layers; Always organize by domain with a small shared `infra/`.
* Never write untyped functions or omit return annotations; Always write pure, typed functions with `->`.
* Never use mutable default arguments or hidden globals; Always pass state explicitly and keep functions pure.
* Never use `assert` for user/data validation; Always perform explicit checks and raise typed exceptions (asserts only for internal invariants).
* Never catch exceptions you can’t handle or use `except:`; Always catch at the innermost layer and wrap/re-raise with domain context.
* Never build deep `if/elif` chains over enums/states; Always use `match/case`.
* Never mutate shared DataFrames in place; Always use vectorized, functional composition that returns new objects.
* Never optimize before measuring; Always profile first and document the context of changes.
* Never rely on nondeterministic randomness; Always set a single global constant seed at process start.
* Never hardcode time/UUID randomness in tests; Always parameterize or fake via dependency injection.
* Never scatter configuration reads across modules; Always load config once at startup, validate eagerly, and inject a typed settings object.
* Never use `None` sentinels for required dependencies; Always fail fast with explicit validation.
* Never print from library code; Always use structured `logging` with appropriate levels.
* Never leave code unexplained; Always precede each 2–5 lines of logic with a “why-first” comment.
* Never skip documentation of public APIs; Always write NumPy-style docstrings with concise examples.
* Never rely on ad-hoc tooling; Always use `uv` with a locked env and `extras:dev` for ruff/pyright/pytest.
* Never let formatting nits block reviews; Always let Autofix CI apply formatting/import-sort and ensure code type-checks locally.
* Never put I/O in the core domain logic; Always isolate side effects in an imperative shell with a functional core.
* Never use `getattr` or `setattr`; Their use is a strong indication of a design flaw.
* Never use dictionaries for structured data; Always use `dataclasses` or Pydantic `BaseModel`.

# [SQL](./sql.md)

* Never SELECT *, Always project just the columns you need.
* Never inline literals into SQL, Always bind parameters.
* Never copy-paste complex/reused queries inline, Always promote them to .sql files.
* Never bury DDL/DML in Python, Always keep migrations and write paths in .sql.
* Never write non-idempotent migrations, Always use IF [NOT] EXISTS and a schema_migrations log.
* Never skip transactions, Always wrap migrations/ETL in a transaction.
* Never rewrite big tables in place, Always stage-then-swap with validation.
* Never materialize views “just because,” Always materialize only for measured latency/SLA wins.
* Never assume an index/layout helps, Always verify with EXPLAIN/ANALYZE.
* Never rely on wide scans and late filters, Always filter early and push predicates down.
* Never move huge raw tables to Pandas to aggregate, Always aggregate in SQL and hand off small results.
* Never default to Python UDFs, Always prefer native SQL; use UDFs sparingly with tests.
* Never mix compute without reason, Always keep set-based ops in SQL and niche math in Pandas/Numpy.
* Never guess data types, Always choose minimal, consistent types at ingest.
* Never overwrite versioned rows, Always use valid_from/valid_to and close the prior version.
* Never drop auditability, Always include created_at, updated_at, source, job_id (and optional row hashes).
* Never allow multiple writers to the same DB file, Always enforce single-writer/many-readers.
* Never hold global connections, Always use context-managed connections per unit of work.
* Never hide critical SQL in code, Always organize .sql under sql/queries, sql/migrations, and sql/etl.
* Never leave query performance undocumented, Always keep brief notes where plans/layout matter.
* Never skip tests, Always cover unit fixtures and property/invariant checks (and optional perf smoke).
* Never ship unreadable SQL, Always uppercase keywords, use snake_case identifiers, and one clause per line.
* Never trust manual formatting, Always let the CI linter/formatter enforce style.
* Never duplicate business logic across queries, Always define a canonical view or query and reuse it.
* Never refresh data blindly, Always validate counts/ranges/hashes before swapping.
* Never expand dynamic SQL freely, Always toggle strictly allow-listed clauses (and still bind values).
################
# filename: contents_dump
################
################
# filename: cheatsheet.md
################
# [Financial Math](./fin_math.md)

* Never weight log returns across assets; Always compute portfolio period return with simple returns (s_rt) then convert to log_rt if needed.
* Never compound across time with products of (1 + s_rt) on long spans; Always sum log_rt and use expm1/exp to get back to simple space.
* Never mix return types implicitly; Always use explicit names: s_rt (simple), log_rt (log).
* Never use np.log(1 + x) or np.exp(x) - 1; Always use np.log1p(x) and np.expm1(x) for stability.
* Never compute returns from unadjusted prices; Always use split/dividend-adjusted prices or add distributions for total return.
* Never forward-fill returns; Always leave NaNs and mask/renormalize weights when data is missing.
* Never assume alignment or local time; Always align indices explicitly and use tz-aware UTC timestamps.
* Never store ledger money as floats; Always use integers in minor units (e.g., cents) or Decimal with fixed quantization (analytics stay float64).
* Never show internal log metrics to users; Always report in simple terms (period s_rt, CAGR) with clear labels.
* Never annualize by arithmetic mean of s_rt × K; Always annualize via mean log_rt then exp(mean_log*K) - 1.
* Never compute volatility with ddof=0; Always use ddof=1 and scale by sqrt(K).
* Never subtract a simple risk-free from log_rt; Always convert RF to per-period log and subtract in log space.
* Never let weights drift without clarity or fail to sum to 1; Always use start-of-period weights, normalize within tolerance, and label series rebalanced vs drifted.
* Never ignore the log1p domain (x > -1); Always guard with epsilon and check finiteness after transforms.
* Never round during analytics; Always round/quantize only at I/O boundaries.
* Never hardcode frequency blindly or mix frequencies; Always carry a frequency tag and use the correct annualization factor (K=252/365/52/12 as policy).
* Never proceed with infs/NaNs silently; Always validate (np.isfinite) and halt or apply an explicit, documented imputation policy.
* Never apply fees/slippage/taxes in log space; Always subtract costs as simple rates at the period, then convert if needed.
* Never rely on implicit index matching for weights/returns; Always assert aligned shapes/indices before arithmetic.
* Never leave compounding semantics ambiguous; Always state the interval convention (left-close/right-open) and what each return maps to (t-1, t]

# [Python](./python.md)

* Never modify `sys.path`; Always place imports at the top, grouped stdlib/third-party/local.
* Never use dynamic, nested, or try/except imports; Always declare explicit dependencies.
* Never use `from __future__ import annotations`; Always use builtin types (e.g., `list[str] | None`).
* Never mix refactors with features; Always separate them into distinct commits.
* Never change behavior without traceability; Always record breaking changes in **CHANGELOG.md › Unreleased** and label the PR.
* Never reference ephemeral or untracked files; Always reference paths present in `git ls-files`.
* Never sprinkle feature flags throughout code; Always read flags once at the edge and inject inward.
* Never organize code by convenience-only layers; Always organize by domain with a small shared `infra/`.
* Never write untyped functions or omit return annotations; Always write pure, typed functions with `->`.
* Never use mutable default arguments or hidden globals; Always pass state explicitly and keep functions pure.
* Never use `assert` for user/data validation; Always perform explicit checks and raise typed exceptions (asserts only for internal invariants).
* Never catch exceptions you can’t handle or use `except:`; Always catch at the innermost layer and wrap/re-raise with domain context.
* Never build deep `if/elif` chains over enums/states; Always use `match/case`.
* Never mutate shared DataFrames in place; Always use vectorized, functional composition that returns new objects.
* Never optimize before measuring; Always profile first and document the context of changes.
* Never rely on nondeterministic randomness; Always set a single global constant seed at process start.
* Never hardcode time/UUID randomness in tests; Always parameterize or fake via dependency injection.
* Never scatter configuration reads across modules; Always load config once at startup, validate eagerly, and inject a typed settings object.
* Never use `None` sentinels for required dependencies; Always fail fast with explicit validation.
* Never print from library code; Always use structured `logging` with appropriate levels.
* Never leave code unexplained; Always precede each 2–5 lines of logic with a “why-first” comment.
* Never skip documentation of public APIs; Always write NumPy-style docstrings with concise examples.
* Never rely on ad-hoc tooling; Always use `uv` with a locked env and `extras:dev` for ruff/pyright/pytest.
* Never let formatting nits block reviews; Always let Autofix CI apply formatting/import-sort and ensure code type-checks locally.
* Never put I/O in the core domain logic; Always isolate side effects in an imperative shell with a functional core.
* Never use `getattr` or `setattr`; Their use is a strong indication of a design flaw.
* Never use dictionaries for structured data; Always use `dataclasses` or Pydantic `BaseModel`.

# [SQL](./sql.md)

* Never SELECT *, Always project just the columns you need.
* Never inline literals into SQL, Always bind parameters.
* Never copy-paste complex/reused queries inline, Always promote them to .sql files.
* Never bury DDL/DML in Python, Always keep migrations and write paths in .sql.
* Never write non-idempotent migrations, Always use IF [NOT] EXISTS and a schema_migrations log.
* Never skip transactions, Always wrap migrations/ETL in a transaction.
* Never rewrite big tables in place, Always stage-then-swap with validation.
* Never materialize views “just because,” Always materialize only for measured latency/SLA wins.
* Never assume an index/layout helps, Always verify with EXPLAIN/ANALYZE.
* Never rely on wide scans and late filters, Always filter early and push predicates down.
* Never move huge raw tables to Pandas to aggregate, Always aggregate in SQL and hand off small results.
* Never default to Python UDFs, Always prefer native SQL; use UDFs sparingly with tests.
* Never mix compute without reason, Always keep set-based ops in SQL and niche math in Pandas/Numpy.
* Never guess data types, Always choose minimal, consistent types at ingest.
* Never overwrite versioned rows, Always use valid_from/valid_to and close the prior version.
* Never drop auditability, Always include created_at, updated_at, source, job_id (and optional row hashes).
* Never allow multiple writers to the same DB file, Always enforce single-writer/many-readers.
* Never hold global connections, Always use context-managed connections per unit of work.
* Never hide critical SQL in code, Always organize .sql under sql/queries, sql/migrations, and sql/etl.
* Never leave query performance undocumented, Always keep brief notes where plans/layout matter.
* Never skip tests, Always cover unit fixtures and property/invariant checks (and optional perf smoke).
* Never ship unreadable SQL, Always uppercase keywords, use snake_case identifiers, and one clause per line.
* Never trust manual formatting, Always let the CI linter/formatter enforce style.
* Never duplicate business logic across queries, Always define a canonical view or query and reuse it.
* Never refresh data blindly, Always validate counts/ranges/hashes before swapping.
* Never expand dynamic SQL freely, Always toggle strictly allow-listed clauses (and still bind values).
################
# filename: contents_dump
################
################
# filename: fin_math.md
################
# Financial Maths Rulebook

This document prescribes **must-follow** engineering practices for modeling returns, performance, and portfolio math in Python. It emphasizes **correctness**, **numerical stability**, and **clear reporting**. It is intentionally opinionated and avoids model-choice debates. Use it as a developer guide or README artifact.

All examples assume `numpy as np` and `pandas as pd`.


## Scope & Principles

Adopt these principles whenever writing analytics code:

1) **Time-series vs cross-section:** Use **log returns** for time aggregation; use **simple returns** for cross-sectional aggregation (e.g., portfolio period returns). Never mix them implicitly.

2) **Numerical stability first:** Prefer `log1p` / `expm1`, test finiteness, and guard domains. Avoid fragile arithmetic.

3) **Explicit naming** of return type: use `s_rt` for **simple** returns and `log_rt` for **log** returns. Never use generic `rt`.

4) **UTC, adjusted prices, typed money:** Use UTC timestamps, fully adjusted prices for return computation, and integer/Decimal for ledger money. Use float64 for analytics arrays.

5) **User-facing clarity:** For end users, prefer **simple returns** and **CAGR** in reports. Internally, compute with log returns when it improves stability and correctness.

6) **Property tests & invariants:** Embed tests that assert identities (e.g., conversion idempotence, portfolio return equals weighted sum of simple returns, etc.).


## Conventions & Types

- **Indexing:** `DatetimeIndex` is **tz-aware UTC**. Rows are time, columns are instruments. Shapes `(T, N)`.

- **Naming:**
  - Prices: `.px`
  - Simple returns: `.s_rt`
  - Log returns: `.log_rt`
  - Weights: `.w`

- **Dtypes:**
  - Ledgers (balances, transactions): **integers in minor units** (e.g., cents) or `Decimal` with fixed quantization.
  - Analytics (prices, returns): **float64**. Round/quantize only at I/O boundaries.

- **Frequency tags:** Maintain a frequency `K` (periods per year) derived from context:
  - Equities: `K=252`
  - Crypto/cash daily: `K=365`
  - Weekly: `K=52`
  - Monthly: `K=12`


## Returns: Definitions & Conversions

**Rule:** Store **log returns** as the canonical time-series representation for analytics; derive simple returns when needed (especially for user-facing reporting and cross-sectional ops).

Recipe (stable conversion):
```
# simple → log (domain guard: s_rt > -1)
eps = 1e-15
assert np.all((df.s_rt.values > -1.0 + eps) | df.s_rt.isna().values)
df["log_rt"] = np.log1p(df.s_rt)

# log → simple
df["s_rt"] = np.expm1(df.log_rt)
```

Anti-patterns:

```
# Wrong or unstable for small x near 0 or -1:
df["log_rt"] = np.log(1 + df.s_rt)      # avoid
df["s_rt"]   = np.exp(df.log_rt) - 1    # less stable than expm1

```
Properties (must hold):

```
# Within numerical tolerance:
np.allclose(np.expm1(np.log1p(df.s_rt)) , df.s_rt, atol=1e-12, equal_nan=True)
np.allclose(np.log1p(np.expm1(df.log_rt)), df.log_rt, atol=1e-12, equal_nan=True)
```


## Cross-Section vs Time Aggregation

**Rule:** Cross-sectional aggregation (assets within the same period) must use **simple returns**. Time aggregation (compounding across periods) should use **log returns**.

Correct period portfolio return:

```
# w_t sums to ~1 at the start of period t; shapes (N,) and (N,)
r_p_t = np.dot(w_t, s_rt_t)  # simple returns only

# If you need the portfolio log return for time aggregation:
log_rt_p_t = np.log1p(r_p_t)
```

Anti-pattern:
```
# Incorrect: summing asset log returns cross-sectionally with weights
# This is a math error for portfolios within a single period.
wrong = np.dot(w_t, log_rt_t)   # DO NOT DO THIS
```

## Cumulative Performance & Reporting

**Rule:** For long horizons, compute cumulative performance via sums of **log returns** (stable) and convert to user-facing simple returns or **CAGR**.

Recipe (cumulative return & wealth path):

```
# Time aggregation: use log returns
cum_log = log_rt.cumsum()
cum_s_rt = np.expm1(cum_log)            # cumulative simple return
wealth   = np.exp(cum_log)              # 1 + cumulative simple return, starting at 1.0
```

User-facing reporting:

```
# Prefer CAGR (annualized geometric mean simple return) for users:
# If mean_log is mean of per-period log returns and K is periods per year:
nn_simple = np.exp(mean_log * K) - 1   # user-facing "annualized return" (CAGR)
```

Anti-pattern:

```
# Over very long series this can accumulate floating error and be less stable:
cum_s_rt_naive = (1 + s_rt).prod() - 1  # permitted, but prefer log path when long

```

## Annualization & Scaling

**Rule:** Use **log-return mean** for annualizing returns; use **std * sqrt(K)** for volatility (either return type, with log preferred for consistency).

Recipes:

```
# Annualized return (CAGR)
mean_log = log_rt.mean()
ann_return = np.exp(mean_log * K) - 1

# Annualized volatility
ann_vol = log_rt.std(ddof=1) * np.sqrt(K)
```

Anti-patterns:

```
# Biased: arithmetic mean of simple returns × K
bad_ann_return = s_rt.mean() * K

# Inconsistent ddof across the codebase
bad_vol = log_rt.std(ddof=0) * np.sqrt(K)  # use ddof=1 for sample stats

```

## Excess Returns & Sharpe

**Rule:** Align risk-free to the same frequency, convert properly, and keep return types consistent.

Recipe:

```
# Given an annualized risk-free simple rate rf_annual (e.g., T-bill):
# Convert to per-period log RF that matches K
rf_per_period_simple = (1 + rf_annual)**(1/K) - 1
rf_per_period_log    = np.log1p(rf_per_period_simple)

# Excess log returns (preferred for Sharpe consistency)
ex_log_rt = log_rt - rf_per_period_log

# Annualized Sharpe
sharpe = ex_log_rt.mean() / ex_log_rt.std(ddof=1) * np.sqrt(K)
```

Anti-pattern:

```
# Mixing types: subtracting simple RF from log returns
wrong_excess = log_rt - rf_per_period_simple   # DO NOT DO THIS
```


## Weights, Rebalancing, Fees

**Rules:**
- Weights are **start-of-period** and sum to 1 within tolerance.
- Default reporting uses **rebalanced** series; provide drifted as a diagnostic.
- Apply fees/slippage/taxes as **simple-return** deductions at the period.

Recipe:

```
# Start-of-period normalization
tol = 1e-9
w_t = w_t / w_t.sum()
assert abs(w_t.sum() - 1.0) < 1e-9

# Net simple return after costs
r_net_t = r_gross_t - cost_rate_t   # cost_rate_t is a simple rate

# Drifted vs rebalanced simulation should be explicitly labeled in outputs
```


## Prices, Corporate Actions & Data Hygiene

**Rules:**
- Use **split- and dividend-adjusted prices** for return computations; otherwise compute total return explicitly (price + distributions).
- Never forward-fill **returns**. Forward-fill **prices** only under explicit, constrained session rules.
- Explicit left-close/right-open labeling: return at `t` maps to `(t-1, t]`.

Recipe (returns from adjusted prices):

```
# Daily simple returns from adjusted prices
s_rt = px_adj.pct_change()
```

Anti-pattern:

```
# Using unadjusted close to compute returns → dividend/split errors
s_rt_wrong = px_unadj.pct_change()   # DO NOT USE FOR PERFORMANCE

```

## Time, Alignment & Missing Data

**Rules:**
- Timestamps are UTC, tz-aware.
- Align series explicitly before arithmetic. Do not rely on implicit alignment with mismatched indices.
- If an asset is missing a price at `t`, its weight should be masked or the return should be NaN; never silently reuse an older value.

Recipe:

```
# Explicit alignment
s_rt_aligned = s_rt.reindex(index=common_index).sort_index()
w_aligned    = w.reindex(index=common_index).sort_index()

# Mask weights where returns are NaN
mask = s_rt_aligned.isna()
w_masked = w_aligned.where(~mask, 0.0)
w_masked = w_masked.div(w_masked.sum(axis=1), axis=0).fillna(0.0)
```


## Ledgers & Money Types

**Rules:**
- **Ledgers:** store money as integers in minor units or `Decimal` with fixed quantization. Never store ledger balances as float.
- **Analytics:** use float64 arrays; convert to Decimal only at I/O boundaries.

Recipe:

```
# Ledger: cents as int; exact summation
txn_amount_cents = 1999  # $19.99
balance_cents += txn_amount_cents
```

Anti-pattern:

```
# Float ledger values subject to rounding error
balance = 0.0
balance += 19.99  # DO NOT DO THIS FOR LEDGERS
```


## Numerical Stability & Guards

**Rules:**
- Always use `log1p` and `expm1` for small quantities; guard the domain for `log1p`.
- Check finiteness (`np.isfinite`) after key transforms and halt or mask with a documented policy.
- Use `ddof=1` for sample statistics; never change ddof silently.

Recipe:

```
eps = 1e-15
assert np.all((s_rt > -1 + eps) | s_rt.isna())
log_rt = np.log1p(s_rt)

assert np.isfinite(log_rt).all() or raise_error(...)
vol = log_rt.std(ddof=1)
```


## Time-Weighted vs Money-Weighted

**Policy:**
- **TWR** is default in dashboards (strategy/portfolio evaluation).
- **(X)IRR** reserved for planning modules with irregular cash flows.
- If both are shown, they must be clearly labeled and described.

Recipe (user-facing):

```
# Reporting: convert internal log stats to simple metrics for clarity
user_cagr = np.exp(log_rt.mean() * K) - 1
user_vol  = log_rt.std(ddof=1) * np.sqrt(K)
```


## Invariants & Property Tests (embed in your test suite)

These properties must hold within tolerances; violations indicate bugs or data quality issues.

```
# 1) Conversion idempotence
assert np.allclose(np.expm1(np.log1p(s_rt)), s_rt, atol=1e-12, equal_nan=True)
assert np.allclose(np.log1p(np.expm1(log_rt)), log_rt, atol=1e-12, equal_nan=True)

# 2) Portfolio period return equals weighted sum of simple returns
r_p_t = (w_t * s_rt_t).sum()
assert np.isscalar(r_p_t)

# 3) Cross-sectional misuse of log returns must fail fast
def portfolio_return_from_log(log_rt_t, w_t):
	raise AssertionError("Cross-sectional portfolio return must use simple returns")

# 4) Annualization identity when K=1
K1 = 1
ann_identity = np.exp(log_rt.mean() * K1) - 1
assert np.allclose(ann_identity, np.expm1(log_rt.mean()), atol=1e-12)

# 5) Rebalance vs drift labeling is preserved
assert "rebalanced" in series_meta or "drifted" in series_meta

# 6) Finite checks at each step
assert np.isfinite(px.values).all()
assert np.isfinite(log_rt.values | np.isnan(log_rt.values)).all()

# 7) Weights sum to 1 (within tolerance) and align with returns
assert abs(w_t.sum() - 1.0) < 1e-9
assert (w.index == s_rt.index).all()
```


## API Ergonomics & Lintable Contracts

**Rule:** Separate APIs by operation type and enforce return-kinds with names and runtime asserts (optionally `typing.NewType`).

Examples:

```
# Time aggregation API (accepts log returns)
def cumulative_from_log(log_rt: pd.Series) -> pd.Series:
	assert "log_rt" in log_rt.name or log_rt.attrs.get("kind") == "log"
	return np.expm1(log_rt.cumsum())

# Cross-sectional API (requires simple returns)
def period_portfolio_s_rt(s_rt_t: pd.Series, w_t: pd.Series) -> float:
	assert "s_rt" in s_rt_t.name or s_rt_t.attrs.get("kind") == "simple"
	assert abs(w_t.sum() - 1.0) < 1e-9
	return float(np.dot(w_t.values, s_rt_t.values))

```

## Positive vs Negative Examples (End-to-End)

**Scenario:** Two assets, daily data, rebalanced portfolio, user-facing performance.

Positive:

```
# 1) Compute simple returns from adjusted prices
s_rt = px_adj.pct_change()

# 2) Convert to log returns for time aggregation
log_rt = np.log1p(s_rt)

# 3) Compute period portfolio simple return (cross-section)
r_p_t = (w_t * s_rt.loc[t]).sum()

# 4) Convert that period return to log for time aggregation
log_rt_p_t = np.log1p(r_p_t)

# 5) Aggregate over time for cumulative performance
cum_s_rt = np.expm1(log_rt_p_t_series.cumsum())

# 6) User-facing metrics
user_cagr = np.exp(log_rt_p_t_series.mean() * 252) - 1
user_vol  = log_rt_p_t_series.std(ddof=1) * np.sqrt(252)
```

Negative:

```
# A) Cross-sectional log-return weighting (wrong)
wrong_r_p_t = (w_t * log_rt.loc[t]).sum()   # math error

# B) Annualized arithmetic mean of simple returns (biased)
wrong_ann = s_rt.mean().mean() * 252        # misleading

# C) Unadjusted prices for returns (incorrect around dividends/splits)
wrong_s_rt = px_unadj.pct_change()

# D) Float ledger balances (rounding drift)
ledger_balance = 0.0
ledger_balance += 19.99                     # not allowed
```


## Reporting Guidance (User-Facing)

- Favor **CAGR** and period **simple returns** in dashboards and summaries.
- Clearly label **rebalanced** vs **drifted** results.
- Avoid jargon where possible; when shown, explain Sharpe and excess returns briefly.
- When internal math used log returns, convert outputs to **simple** metrics for display and narrative clarity.


## Checklist for PR Review (interpret, not copy-paste)

- Are cross-sectional operations using **simple** returns only?
- Are time aggregations using **log** returns with `log1p`/`expm1`?
- Are price series **adjusted** and time indexes **UTC**?
- Are ledgers **integer/Decimal**, analytics **float64**?
- Are annualization formulas consistent (CAGR via mean log, vol via sqrt(K))?
- Are user-facing figures in **simple** terms (CAGR, simple returns) and clearly labeled?
- Do tests assert the properties listed above?


## Summary

- **Cross-section:** simple returns.
- **Time:** log returns.
- **Reporting:** user-friendly simple metrics (CAGR, period simple returns) with clear labels.
- **Stability:** `log1p`/`expm1`, finiteness checks, adjusted prices, UTC time.
- **Money:** integer/Decimal for ledgers, float64 for analytics.
- **Tests:** property-based guardrails to catch misuse early.

Adhering to these rules prevents common analytics bugs, maintains numerical stability, and ensures users see accurate, interpretable performance figures.

################
# filename: promptify
################
#!/usr/bin/env bash

for file in $(ls $1); do
    echo "################"
    echo "# filename: $file"
    echo "################"
    cat $file
done
################
# filename: python.md
################
# Python Best Practices & Style Guide (Codebase-Wide)

This guide emphasizes maintainability, clarity, correctness, and determinism over cleverness. 


## Naming & Organization

Organize packages by **domain responsibility** with a shared `infra/` (or `platform/`) for true cross-cutting utilities. New code belongs **next to the closest responsible module**.

Prefer the **smallest reasonable change**. If you must refactor, isolate it in its own commit (no mixed refactor + feature).

Examples of structure:

    core/
        db.py
        logging.py
    orders/
        routers.py
        service.py
    pricing/
        router.py
        service.py
        discounts.py



## Compatibility & Change Control

We are at v0; breaking changes are allowed. Every breaking change must:

1. Add a bullet under **CHANGELOG.md › Unreleased › Breaking Changes** with a one-line migration note.  
2. Apply the `breaking-change` label on the PR.

Feature flags are **read once at the edge** (config load) and **passed inward**; do not sprinkle flags across modules or read the same flag in multiple places.



## Imports & Dependencies

Imports are **at the top of the file**, grouped and ordered per PEP8: stdlib, third-party, local. Never modify `sys.path`; no dynamic/nested/try-except imports; dependencies must be declared and expected to exist.

**Example — import grouping**

    import pathlib
    from dataclasses import dataclass

    import numpy as np
    import pandas as pd

    from orders.dao import OrderDAO



## Types & Functions

Write **pure, typed functions** by default and **always** include return annotations. Prefer builtin type forms (`list[str] | None`), avoid `typing` aliases unless necessary. Avoid hidden globals and mutable default arguments.

Use `assert` **only** for internal invariants that may be stripped with `-O`. Use explicit validation with typed exceptions for user/data errors.

**Example — type annotations and validation**

    def top_n(xs: list[int], n: int) -> list[int]:
        # Why: we require a positive n and do not allow n > len(xs) to prevent surprises.
        if n <= 0:
            raise ValueError("n must be positive")
        if n > len(xs):
            raise ValueError("n must not exceed length of xs")
        return sorted(xs, reverse=True)[:n]

**Anti-pattern**

    def add_user(u: dict = {}):    # BAD: mutable default
        ...



## Control Flow & Exceptions

Prefer `match/case` when branching on enums/tagged states. Catch exceptions at the **innermost layer that can actually handle them**. Never use `except:` (bare). Wrap third-party exceptions at boundaries with domain exceptions; do not silence failures.

**Example — pattern matching**

    def status_to_message(code: int) -> str:
        # Why: pattern matching keeps branching flat and readable.
        match code:
            case 200:
                return "ok"
            case 404:
                raise NotFound("resource not found")
            case _:
                raise UnexpectedStatus(code)

**Example — exception handling**

    def load_order(order_id: str) -> dict:
        # Why: only this layer knows how to map persistence errors to domain errors.
        try:
            return OrderDAO().get(order_id)
        except DatabaseTimeout as exc:
            # What: convert infra error to domain exception with context and re-raise.
            raise OrderUnavailable(f"Order {order_id} temporarily unavailable") from exc



## Statics over Dynamics

We **always** prefer simple, static, type-safe behavior over dynamic features. Code should be explicit and predictable.

- **`getattr` / `setattr` are forbidden.** Their use is a strong indication of a design flaw.
- **Prefer objects over dictionaries for structured data.** Instead of dictionaries with string keys, use `dataclasses` or Pydantic `BaseModel` to define clear, typed contracts for your data structures. This improves readability, enables static analysis, and prevents runtime errors from typos in keys.

**Example — typed data object**

    from dataclasses import dataclass

    @dataclass(frozen=True)
    class UserProfile:
        user_id: str
        email: str
        is_active: bool

    # GOOD: Clear, typed, and predictable.
    def process_user(profile: UserProfile):
        ...

**Anti-pattern**

    # BAD: Dynamic, error-prone, and hard to reason about.
    def process_user(profile: dict):
        if profile["is_active"]:  # Prone to typos, no static analysis.
            ...



## Numerics, DataFrames, and Performance

Favor **NumPy/Pandas vectorization** and **functional composition**. Think Rust: **avoid aliasing mutable data**; use a single “owner” for a frame; copy at boundaries if needed and return new objects.

**Example — functional style with DataFrames**

    def add_margin(df: pd.DataFrame, pct: float) -> pd.DataFrame:
        # Why: avoid in-place mutation of shared frames; composition-friendly.
        return df.assign(margin=lambda d: d["price"] * pct,
                         total=lambda d: d["price"] + d["price"] * pct)

Measure first, optimize second; do not micro-optimize without evidence.

**Light performance rubric**

1. Identify candidate hot paths with simple timers (`time.perf_counter()` in a focused micro-bench description).  
2. Consider memory behavior when copying vs mutating.  
3. Only drop to Python loops or cython/numba after profiling shows a bottleneck.



## Comments, Docstrings, and Logging

Every **2–5 lines** of logic are preceded by a **why-first** comment; add what/explanation where non-obvious (complexity, domain, institutional knowledge). Keep comments in sync with code.

Use **NumPy-style** docstrings, with concise examples. No `print` in library code; use `logging` with structured context.

**Example — NumPy docstring**

    def normalize(x: np.ndarray) -> np.ndarray:
        """
        Normalize a vector to unit L2 norm.

        Parameters
        ----------
        x : np.ndarray
            Input vector.

        Returns
        -------
        np.ndarray
            Normalized vector where ||x||_2 == 1.

        Examples
        --------
        >>> normalize(np.array([3.0, 4.0]))
        array([0.6, 0.8])
        """
        # Why: normalize ensures downstream algorithms assume consistent scale.
        norm = float(np.linalg.norm(x))
        if norm == 0:
            raise ValueError("cannot normalize zero vector")
        return x / norm



## Testing & Quality Gates

Adopt **unit-first** tests with deterministic fixtures and **property-based tests** for invariants. Keep tests fast and deterministic.

CI runs **ruff** (lint/imports) + **black** (format) + **pyright** (fast type checks). **Autofix CI** will apply formatting/lint changes; developers may run tools locally but are primarily responsible for ensuring code **type checks** quickly. Developers fix lint/format locally only if autofix fails.

Flaky tests: quarantine upon first verified flake; file an issue; fix before next release.



## Configuration, Packaging, and Versioning

Load config from environment/`.env` **once at process start**; validate eagerly into a **typed settings object**. Do not use `None` sentinels for required dependencies; inject settings explicitly.

Dependencies are managed with **uv**. Use `extras:dev` for ruff/pyright/pytest; allow additional extras sparingly for optional features. Use **SemVer**. Update **CHANGELOG.md** per PR under **Unreleased**, reverse chronological.



## Determinism & Seeding

A single **global constant seed** is set at process start. Imports remain at top-of-file. No nested or lazy imports. Do not mutate the seed.

**Example — deterministic seeding (entrypoint)**

    # stdlib
    import os
    import random

    # third-party
    import numpy as np

    # local
    from platform.settings import SETTINGS  # carries SEED as an int constant

    SEED: int = SETTINGS.SEED  # immutable by convention

    # Why: establish deterministic behavior across Python, NumPy, and hash randomization.
    os.environ["PYTHONHASHSEED"] = str(SEED)
    random.seed(SEED)
    np.random.seed(SEED)



## CI Automation

CI executes linters, formatters, and type checks. An autofix bot pushes non-semantic changes (format/import-sort). Reviews are not blocked by formatting nits.

Optionally, CI may warn on forbidden file paths by comparing string/path literals against the `git ls-files` allowlist. This starts as a warning until we confirm low false positives.



## Examples & Anti-Patterns

**Import grouping (good)**

    import logging
    from pathlib import Path

    import numpy as np
    import pandas as pd

    from pricing.engine import price_quote

**Type annotations (good)**

    def pct_change(xs: np.ndarray) -> np.ndarray:
        # Why: vectorized computation is faster and easier to test.
        if xs.ndim != 1:
            raise ValueError("xs must be a 1D array")
        return xs[1:] / xs[:-1] - 1.0

**Pattern matching (good)**

    def classify(kind: str) -> int:
        # Why: explicit, flat branching; easy to extend.
        match kind:
            case "retail":
                return 1
            case "wholesale":
                return 2
            case _:
                raise ValueError(f"unknown kind: {kind}")

**Exception handling (good)**

    def parse_price(s: str) -> float:
        # Why: handle only the error we can remediate; keep message actionable.
        try:
            return float(s)
        except ValueError as exc:
            raise InvalidPrice(f"invalid price: {s!r}") from exc

**Anti-patterns (never do this)**

    # BAD: nested import
    def foo():
        import numpy as np  # ❌ forbidden
        ...

    # BAD: bare except
    try:
        ...
    except:  # ❌ forbidden
        ...

    # BAD: mutable default
    def f(cfg: dict = {}):  # ❌ forbidden
        ...

    # BAD: referencing ignored/ephemeral paths
    Path("/tmp/data.csv").read_text()  # ❌ use tracked paths only



## Performance Guidance

Measure first, then optimize. Describe the context of any micro-bench (input size, machine hints). Prefer vectorization and composition; only reach for loops/numba/cython if profiling identifies a hotspot. When copying DataFrames, be deliberate about memory; document the tradeoff in a short comment.



## Refactor vs Feature — Commit Example

**Refactor (separate commit)**

    # Why: isolate rename and function extraction to keep feature diff small.
    chore(pricing): extract margin calc; rename 'calc' -> 'compute_margin'

    - Extract compute_margin(df, pct) from engine.py
    - Rename calc -> compute_margin across pricing/
    - No behavior change (verified by tests)

**Feature (follow-up commit)**

    feat(pricing): add discounted total to quotes

    - New field 'discounted_total' computed via compute_margin + discount rules
    - Update tests for quote serialization

This sequencing preserves review clarity and minimizes risk.



## Prohibitions (Hard-Fail in CI)

- Never modify `sys.path`.  
- No dynamic imports, no nested imports, no `try/except` imports.  
- No `from __future__ import annotations`.  
- No `getattr` or `setattr`.
- No `print` in library code.  
- No `except:` (bare) or catching exceptions that cannot be handled locally.  
- No hidden globals or mutable default arguments.  
- No premature optimization (unjustified micro-tweaks).  
- No references to ephemeral or non-tracked files; prefer the `git ls-files` allowlist.  
- No mixed refactor + feature in the same commit.



## Closing Notes

Prefer domain-responsibility naming, keep changes minimal, write pure typed functions with clear return annotations, validate inputs explicitly, handle exceptions where they can be resolved, choose `match/case` for stateful branching, favor vectorization and functional composition, and keep the system deterministic via a single seed at process start. Comments explain **why**. CI automates hygiene so reviews focus on behavior and design.


################
# filename: sql.md
################
SQL Best Practices for Python Codebases


## Scope and Intent

This document sets a pragmatic standard for writing, organizing, testing, and operating SQL—specifically with DuckDB—inside a Python project. It aims to be actionable for everyday engineering and durable enough to live in developer docs or a README. Where practices involve judgment, we provide clear decision criteria, examples, and anti-patterns. All guidance assumes a modest concurrency environment and that DuckDB tables are the curated data store for the application.


## Guiding Principles

Write SQL that is explicit, testable, and composable. Prefer set-based transformations, keep data close to the compute, and evolve schemas with idempotent migrations guarded by transactions. Optimize for pushdown and minimal data movement. Make correctness measurable with unit and property tests. Where performance matters, optimize layout and materialization—not just query text.


## Where SQL Lives (Hybrid: Inline + Files)

Small, local queries may live inline in Python; reusable or complex queries must be first-class `.sql` files.

Promotion rules (when a query “graduates” to a `.sql` file):

- Reuse: The query is used in ≥2 places or call sites.
- Complexity: The query has a CTE and a join, or uses window functions, or spans > ~12 logical lines.
- Contract-bearing: Downstream jobs depend on its exact columns/semantics (e.g., a canonical view).
- Performance-tuned: The query required EXPLAIN/ANALYZE work, layout assumptions, or materialization.
- Write paths & migrations: All DDL and DML for ETL/migrations live in `.sql`.
- Security/policy sensitive: The query enforces business rules or filters PII/roles.

Inline is acceptable when the query is ≤ ~6 lines, read only, and tightly scoped to nearby code (e.g., fetching a single row by key).

Recommended layout:

- sql/queries/<domain>/<name>.sql
- sql/migrations/VYYYYMMDD__<slug>.sql
- sql/etl/<job>/<step>.sql


## Parameterization & Composition

Use pure SQL with bound parameters (no free-form string interpolation):

- Bind parameters from Python rather than composing literals into SQL.
- Keep dynamic SQL limited to optional clauses that are safely toggled (e.g., adding a filter only if present).
- Avoid heavy templating; if you must, keep a strict allow-list for identifiers and never inline raw values.

Example (good):

    con.execute(
      "SELECT id, amount FROM payments WHERE merchant_id = ? AND date >= ?",
      [merchant_id, start_date]
    )

Anti-pattern (unsafe):

    con.execute(f"SELECT id FROM payments WHERE merchant_id = {merchant_id}")


## Performance in DuckDB: Posture and Heuristics

DuckDB executes vectorized, columnar plans with strong predicate/projection pushdown. Speed comes from reading fewer columns/rows, minimizing materializations, and maintaining helpful sort/layout characteristics.

Adopt these in priority order:

1) Pushdown-first (always)
- Select explicit columns; avoid SELECT * in app code.
- Filter early; push predicates into the scan when possible.
- Prefer EXISTS over IN for correlated membership checks.

2) Data-layout aware (for curated tables)
- Create/refresh curated tables with intentional ordering to support common groupings/filters.
- Choose compact types and consistent encodings; avoid unnecessary casting churn.
- Pre-aggregate hot rollups used in dashboards or repetitive reports.

3) Views + cache tables (selectively)
- Define canonical views for logic clarity.
- Materialize heavy views as tables where latency/SLA requires; refresh via stage-then-swap (see Transactions).

About DuckDB indexes:

- DuckDB supports explicit indexes (e.g., on sort keys or selective predicates). Use them judiciously for point lookups or selective filters on curated tables.
- For analytics, sorted/clustered layout plus pushdown typically dominates; verify index benefit with EXPLAIN/ANALYZE, not assumption.

Minimal EXPLAIN habit:

- Keep a small fixture dataset and occasionally verify that expensive queries exhibit predicate/column pushdown and avoid unnecessary re-scans.


## Compute Placement: SQL vs Pandas/Numpy vs Python UDFs

Default to SQL for set-based operations; use Pandas/Numpy for specialized math on small, post-aggregated frames; reach for Python UDFs sparingly.

Decision cues:

- Joins, filters, group-bys, window functions, pivots: SQL.
- Iterative algorithms, specialized numeric routines, branchy row-wise transforms: reduce in SQL first, then Pandas/Numpy on the smaller result.
- Must stay in SQL but need custom logic: consider a Python UDF only when its scope is small and measured.

Examples:

- Good (pushdown first):

    -- Curate minimal columns before Python
    WITH base AS (
      SELECT order_id, merchant_id, dispatch_date, total_cents
      FROM orders
      WHERE dispatch_date BETWEEN ? AND ?
        AND merchant_id = ?
    )
    SELECT merchant_id, dispatch_date, SUM(total_cents) AS gross_cents
    FROM base
    GROUP BY merchant_id, dispatch_date

- Anti-pattern (shipping the world to Python):

    -- Avoid selecting wide fact tables to Pandas only to re-group there
    SELECT * FROM orders


## Migrations & Schema Evolution (SQL-First, Python-Orchestrated)

All migrations are idempotent `.sql` files executed by a thin Python runner that adds pre/post assertions and transaction control.

Practices:

- Use IF NOT EXISTS/IF EXISTS where appropriate.
- Maintain a schema_migrations table recording applied versions.
- Wrap each migration in a transaction; fail closed.
- Add pre-checks (does input table exist? columns?) and post-checks (row counts, nullability).
- Keep data backfills in separate, explicit steps.

Example (idempotent DDL):

    BEGIN;
    CREATE TABLE IF NOT EXISTS payments (
      id            BIGINT PRIMARY KEY,
      merchant_id   INTEGER NOT NULL,
      amount_cents  BIGINT NOT NULL,
      created_at    TIMESTAMP NOT NULL,
      updated_at    TIMESTAMP NOT NULL
    );
    ALTER TABLE payments
      ADD COLUMN IF NOT EXISTS currency VARCHAR NOT NULL DEFAULT 'USD';
    COMMIT;

Anti-pattern:

    -- Blind “CREATE TABLE” without IF NOT EXISTS; no transaction; no checks.


## Transactions & Safety (Stage-Then-Swap)

Default to transactions for small updates; use stage-then-swap for heavy rewrites or refreshes to avoid partial reads.

Pattern:

- Write results to table__staging.
- Validate: counts, sums/hashes, min/max ranges.
- Swap inside a single transaction:

    BEGIN;
    DROP TABLE IF EXISTS table__backup;
    ALTER TABLE table RENAME TO table__backup;
    ALTER TABLE table__staging RENAME TO table;
    COMMIT;

If validation fails, ROLLBACK and investigate. Prefer a dedicated staging schema or a naming convention; keep swaps predictable and scripted.


## Testing Strategy (Unit + Properties)

Unit tests (fixtures):

- Use an in-memory DuckDB or a temporary file.
- Seed small, deterministic fixtures (including nulls and duplicates).
- Assert schema, key uniqueness, specific cell values, and edge-case behavior.

Property tests (invariants):

- Key uniqueness and non-negativity, conservation (credits = debits), monotonic timestamps, no future dates, controlled null rates, bounded ranges.
- For floats, use explicit tolerances.

Optional (nice to have):

- Perf smoke on fixtures to catch accidental quadratic plans or missing pushdown.


## Data Modeling & Layout

We assume curated data lives in DuckDB tables (no external Parquet). Optimize the layout you materialize:

- Define surrogate keys where appropriate; document business keys.
- Choose minimal column types; avoid accidental BIGINT/DOUBLE creep.
- For hot tables, load or rebuild with ORDER BY on frequent group/filter keys.
- Consider summary marts (daily/weekly rollups) for repeated aggregations, refreshed via stage-then-swap.

Example (curated rebuild):

    CREATE OR REPLACE TABLE orders_curated AS
    SELECT
      CAST(id AS BIGINT)               AS order_id,
      CAST(merchant_id AS INTEGER)     AS merchant_id,
      CAST(total_cents AS BIGINT)      AS total_cents,
      CAST(dispatch_date AS DATE)      AS dispatch_date,
      created_at,
      updated_at
    FROM orders_raw
    WHERE status = 'complete'
    ORDER BY merchant_id, dispatch_date;


## Observability: Audit Columns and Temporal (Valid-Time) Tables

Audit columns:

- created_at, updated_at, source, job_id, version, and optionally a row_hash for integrity.
- Populate/refresh in ETL steps; never backfill silently.

Valid-time (SCD2-style) tables:

- Add valid_from (NOT NULL) and valid_to (NOT NULL, sentinel '9999-12-31'), and a generated is_current flag.
- On change, close the prior version (valid_to = now()) and insert a new row with valid_from = now().
- Point-in-time queries filter: valid_from <= t AND valid_to > t.

Example (upsert pattern):

    -- Close current version
    UPDATE user_config
      SET valid_to = now(), updated_at = now()
    WHERE user_id = ?
      AND valid_to = '9999-12-31';

    -- Insert new version
    INSERT INTO user_config (
      user_id, setting_a, setting_b,
      valid_from, valid_to, created_at, updated_at, source, job_id
    )
    VALUES (?, ?, ?, now(), '9999-12-31', now(), now(), ?, ?);

Anti-pattern:

    -- Overwriting “current” rows in place without closing valid_to; breaks time travel.


## Style & Readability

- Keywords UPPERCASE; identifiers snake_case; short, meaningful aliases.
- One major clause per line; break long SELECT lists across lines.
- Never use SELECT * in application code; be explicit.
- Comment only non-obvious logic; keep comments high-signal.

Example (readable):

    SELECT
      o.order_id,
      o.merchant_id,
      o.dispatch_date,
      SUM(oi.ext_price_cents) AS gross_cents
    FROM orders o
    JOIN order_items oi ON oi.order_id = o.order_id
    WHERE o.dispatch_date BETWEEN ? AND ?
    GROUP BY o.order_id, o.merchant_id, o.dispatch_date


## Tooling & CI

- Use a SQL formatter/linter in CI to enforce style and safe patterns; allow an autofix bot to apply formatting so it doesn’t block human review.
- Keep unit/property tests mandatory in CI; optionally include a small EXPLAIN/perf smoke.
- Treat formatting as automation: the doc states rules; the linter encodes them.

Practical tips:

- Mark inline SQL strings clearly (e.g., with a preceding comment) so linters can target them.
- Fail CI on unsafe patterns (e.g., SELECT * in app code) and on parsing errors.


## Patterns Library (Suggested Contents)

Keep short, prose-first pattern pages with a rationale and a compact example:

- Surrogate key generation and de-duplication.
- Stage-then-swap refresh workflow.
- Temporal (valid-time) table upserts and time-travel queries.
- Window function idioms: first/last per group, retention, rolling metrics, percentiles.
- EXISTS vs IN: when and why EXISTS tends to outperform for correlated checks.
- Materialization cues: when to cache a view into a table and how to refresh safely.


## Examples & Anti-Patterns

Selective retrieval (good):

    SELECT id, created_at, amount_cents
    FROM payments
    WHERE merchant_id = ?
      AND created_at >= ?
    ORDER BY created_at DESC
    LIMIT 500

Anti-pattern: broad scans and late filters:

    -- Selects everything, then filters later in Python
    SELECT * FROM payments

Join ordering for clarity and pushdown (good):

    SELECT
      m.merchant_id,
      d.dispatch_date,
      SUM(o.total_cents) AS gross_cents
    FROM merchants m
    JOIN orders o ON o.merchant_id = m.merchant_id
    JOIN dispatches d ON d.order_id = o.order_id
    WHERE d.dispatch_date BETWEEN ? AND ?
    GROUP BY m.merchant_id, d.dispatch_date

Anti-pattern: mixing logic between SQL and Python without reason:

    -- Compute partial sums in SQL, finish aggregations in Python for the same grain
    -- (push the full aggregation into SQL unless you truly need Python-only math)


## FAQ-Style Notes

- “Do we need indexes?”  
  Consider them when repeated selective filters benefit measurably; validate with EXPLAIN/ANALYZE. For many analytics workloads, sorted layout + pushdown + pre-aggregation yield bigger wins.

- “When do we cache a view as a table?”  
  When it’s hot, latency-sensitive, or composes several joins/windows repeatedly. Refresh with stage-then-swap and test the row counts/hashes.

- “When should I move code from Pandas back to SQL?”  
  If the Pandas step starts reading large frames only to do group-bys/joins/filters, move that work into SQL and return a small result to Python.


## Closing

Favor explicit, pushdown-friendly SQL and intentional data layout. Keep migrations idempotent and guarded by transactions. Test correctness with unit and property checks. Materialize selectively when performance or stability requires it. Use automation to enforce style; use patterns to share the winning playbooks.

This standard should evolve with real metrics and post-mortems—tune the rules where evidence merits, but keep the core principles: explicitness, testability, and pushdown.

